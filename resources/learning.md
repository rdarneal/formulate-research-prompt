# Learning & Instructional Design Research Template

Use for: training programs, workshops, executive education, capability building, curriculum design.

## Template

```
LEARNING & INSTRUCTIONAL DESIGN RESEARCH PROMPT

=== 1. LEARNING CONTEXT & FRAMEWORK ===

Expert(s) conducting the research: [For complex topics, specify multidisciplinary team with 2-4 specific roles, e.g., "L&D Director with adult learning specialization, Subject matter expert in [domain], Instructional designer with digital learning expertise, Assessment specialist"]
Learning Domain: [Subject area and organizational context, e.g., "Data literacy for non-technical managers," "Leadership development for mid-level engineers"]
Research Objective: [What this research must answer or produce — curriculum, program design, assessment strategy, etc.]
Current Understanding: [What is known about the learners, existing programs, organizational readiness, and current skill gaps]
Knowledge Gaps: [What is unknown — skill baselines, learner preferences, effectiveness of current approaches, transfer barriers]
Potential Impact: [How findings will influence learning outcomes, performance, business results, retention, etc.]

=== 2. CORE LEARNING QUESTION & HYPOTHESIS ===

Primary Learning Question: [Single, precise question this research centers on]
Working Hypothesis: [Best current assumption about effective approach, with specific mechanisms]
Alternative Approaches: [2-3 competing instructional strategies worth evaluating]
Theoretical Foundation: [Learning theories grounding this inquiry, e.g., Bloom's Taxonomy, Kolb's Experiential Learning, Cognitive Load Theory, ADDIE, SAM, 70-20-10 model, social learning theory]

=== 3. METHODOLOGICAL PARAMETERS ===

Instructional Design Model: [e.g., ADDIE, SAM, Backward Design, Design Thinking for Learning, Agile Learning Design]

Learner Profile:
  - Audience: [Who — role, level, background, e.g., "Mid-level product managers with 3-5 years experience"]
  - Group size: [Expected cohort size or range]
  - Prerequisites: [Required prior knowledge or skills]
  - Motivation context: [Voluntary vs mandatory, career incentives, pain points, learning culture]

Session Constraints: [Duration, frequency, delivery mode (in-person, virtual, async, blended), time zone considerations]
Instructional Methods: [e.g., case studies, simulations, peer learning, microlearning, project-based learning, cohort-based courses, on-the-job application]
Assessment Methods: [e.g., pre/post tests, performance rubrics, portfolio review, manager observation, 360 feedback, Kirkpatrick levels, skills demonstration]

=== 4. LEARNING OUTPUT SPECIFICATIONS ===

Report Structure:
  - Executive summary: [Include? Yes/No. If yes, specify length]
  - [List other required sections based on the specific design needs]
  - [Examples: Learner needs analysis, Learning objectives (mapped to business outcomes), Curriculum design/program architecture, Assessment and evaluation plan, Facilitation guide or delivery notes, Resource and tool requirements, Appendices (references, templates)]

Analytical Depth:
  [ ] Level 1: Program overview for stakeholder approval
  [ ] Level 2: Detailed curriculum with session plans
  [ ] Level 3: Full instructional package with materials and scripts

Required Elements (check only what's essential for THIS research):
  [ ] Learning objectives (SMART or competency-based)
  [ ] Curriculum map or program structure
  [ ] Session/module outlines with timing
  [ ] Assessment strategy (formative and summative)
  [ ] Learner engagement and interaction techniques
  [ ] Facilitator notes or delivery guide
  [ ] Evaluation plan (Kirkpatrick or equivalent)
  [ ] Materials list and resource requirements
  [ ] Technology requirements and platform recommendations
  [ ] Transfer of learning strategy (application planning)

Visualization Requirements: [e.g., curriculum maps, competency matrices, learning journey diagrams, assessment rubrics, session flow diagrams]
Target Audience: [e.g., "L&D team and business unit sponsors," "Executive stakeholders," "Facilitators and SMEs"]
Citation Style: [e.g., APA for academic rigor, inline links for practitioner use, or "include reference list"]

=== 5. EVIDENCE HIERARCHY & SOURCE QUALITY ===

Source Priority:
  - Tier 1 (Primary Evidence): [e.g., peer-reviewed education research, meta-analyses, randomized controlled trials, longitudinal studies]
  - Tier 2 (Market Intelligence): [e.g., ATD/CIPD publications, Harvard Business Publishing cases, established practitioner books, corporate learning research]
  - Tier 3 (Contextual): [e.g., practitioner blogs, conference presentations, internal pilot data, case studies from similar organizations]

Source Exclusions: [e.g., "No vendor-sponsored 'research' without independent validation," "No sources older than 5 years for digital learning methods," "No unverified training fads"]
Cross-Domain Insights: [Adjacent fields or analogous approaches worth mining, e.g., "Behavioral economics for nudge-based learning design," "UX research for learner experience," "Gamification from game design principles"]

=== 6. QUALITY ASSURANCE & REPRODUCIBILITY ===

Evidence Grading: [How to assess confidence — e.g., "Rate confidence in each instructional recommendation: High (empirically validated) / Medium (best practice consensus) / Low (expert opinion or pilot data only)"]
Reproducibility: [Requirements for delivery, e.g., "Include session plans, materials lists, facilitation scripts, timing guidance so another facilitator could deliver the program with fidelity"]
Bias Assessment: [Specific biases to watch for, e.g., "Flag assumptions about learner motivation, transfer of learning, organizational support, technology access, learning styles myths"]
Uncertainty Quantification: [What to make explicit, e.g., "State where evidence is thin and recommendations are based on best practice rather than empirical proof for this specific context"]

=== 7. RESEARCH VALIDATION PROTOCOL ===

Negative Results: [How to document rejected approaches, e.g., "Explicitly address what instructional approaches were considered and rejected, with specific reasoning"]
Alternative Interpretation: [Perspective-taking requirement, e.g., "For key design decisions, document what a proponent of a different instructional approach would argue"]
Peer Review Simulation: [Objection handling, e.g., "Anticipate the strongest objections an experienced instructional designer would raise and address them proactively"]
Replication Strategy: [Pilot and iteration plan, e.g., "Describe how the program could be piloted with a small cohort, measured against success criteria, and refined before scaling"]
```